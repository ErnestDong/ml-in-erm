\documentclass[mathserif,envcountsect,fontset=mac]{ctexbeamer}
\usepackage[backend = biber, style = gb7714-2015, url=false,gbtitlelink=true]{biblatex}
\addbibresource{ref.bib}
\usepackage{xeCJK}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{minted}
\usepackage{hyperref}
\usetheme{CambridgeUS}
\author{董晨阳}
\date{\today}
\title{机器学习在企业风险管理中的应用举例}
\setbeamertemplate{bibliography item}{}
\setbeamertemplate{bibliography entry article}{}
\setbeamertemplate{bibliography entry title}{}
\setbeamertemplate{bibliography entry location}{}
\setbeamertemplate{bibliography entry note}{}
\setbeamertemplate{caption}[numbered]
\begin{document}

\AtBeginSection[]
{
    \small\begin{frame}
        \frametitle{目录}
        \tableofcontents[
            sectionstyle=show/shaded,
            subsectionstyle=show/show/hide,
            subsubsectionstyle=show/show/show/hide
        ]
    \end{frame}
}
\maketitle
\begin{frame}
    \frametitle{目录}
    \tableofcontents[hideallsubsections]
    \small 笔记见 \url{https://ernestdong.github.io/posts/machine_learning_in_erm/}，

    \small 源代码见 \url{https://github.com/ErnestDong/ml-in-erm}
\end{frame}

\section{前言}
\subsection{机器学习概述}
\begin{frame}
    \frametitle{何谓“机器学习”}
    \begin{columns}
        \column{0.6\linewidth}
        \includegraphics[width=.9\textwidth]{/Users/dcy/Code/ernest/static/images/xkcd/1838.png}
        \column{0.35\linewidth}
        什么是学习？

        \href{https://zh.wikipedia.org/wiki/\%E5\%AD\%A6\%E4\%B9\%A0}{维基百科}上说学习是获得新的理解、知识、行为、技能、价值观、态度和偏好的过程。

        在计算技术快速发展的今天，让机器去利用算法和算力去“学习”、推理、决策，就是机器学习。

        \begin{definition}
            机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。
        \end{definition}
    \end{columns}
\end{frame}
\subsection{机器学习的分类}
\begin{frame}
    \frametitle{机器学习的分类}
    \begin{columns}
        \column{0.6\linewidth}
        \includegraphics[width=.9\textwidth]{../lib/机器学习.jpeg}
        \column{0.35\linewidth}
        机器学习深究的话，需要学习很多数学和计算机知识。
        但是工业界将常用的机器学习算法封装地很好（如pytorch, scikit-learn），几行代码就可以实现一个模型。

        本文主要参考了\textcite{scikit-learn}的文档，在编码过程中阅读文档是有帮助的。
    \end{columns}
\end{frame}
% \subsection{一点建议}
% \begin{frame}
%     \frametitle{要学习机器学习吗？}
%     \begin{itemize}
%         \item 了解一下常见算法的思想和适用场景
%         \item 学好专业课和数学
%         \item 有问题查文档
%     \end{itemize}
% \end{frame}
\section{机器学习预测信用评级}
\subsection{数据预处理}
\begin{frame}
    \frametitle{数据来源}
    数据来自 \href{https://www.kaggle.com/datasets/agewerc/corporate-credit-rating}{kaggle}。
    涵盖了 2029 家美国上市公司信用评级的历史数据。数据除了公司基本信息外，还包括了30个财务特征：

    \begin{enumerate}
        \item 流动性指标: currentRatio, quickRatio, cashRatio, daysOfSalesOutstanding
        \item 盈利能力: grossProfitMargin, operatingProfitMargin, pretaxProfitMargin, netProfitMargin, effectiveTaxRate, returnOnAssets, returnOnEquity, returnOnCapitalEmployed
        \item 负债比率: debtRatio, debtEquityRatio
        \item 营运表现: assetTurnover, fixedasset
        \item 现金流指标: operatingCashFlowPerShare, freeCashFlowPerShare, cashPerShare, operatingCashFlowSalesRatio, freeCashFlowOperatingCashFlowRatio
    \end{enumerate}
\end{frame}
\begin{frame}[fragile]
    \frametitle{数据简单处理}
    评级分布如图\ref{rating}所示。
    我们会合并 C/CC/CCC 的评级，选取 3/4 作为训练集，1/4 作为测试集。
    \begin{figure}
        \includegraphics[width=0.6\linewidth]{../lib/rating.png}
        \label{rating}
        \caption{评级分布}
    \end{figure}
\end{frame}
\begin{frame}[fragile]
    \frametitle{数据简单处理}
    \begin{minted}{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df = pd.read_csv("./corporate_rating.csv", encoding="utf-8")
Y = df["Rating"].replace({"CCC": "C", "CC": "C"})
df["Date"] = df["Date"].apply(lambda x: x.split("/")[-1])
dummies = ["Rating Agency Name", "Sector", "Date"]
X = df[[i for i in df.columns if df[i].dtype != "object"]]
X = pd.concat([X]+[pd.get_dummies(df[i], drop_first=True)
    for i in dummies]),
    axis=1)
Xtrain, Xtest, Ytrain, Ytest = train_test_split(
    X, Y, test_size=0.25, random_state=42
)
\end{minted}
\end{frame}
\begin{frame}
    \frametitle{评价机器学习效果的指标}
    对于二分类问题，一个样本真实情况可能是 True/False，对应预测可能是 Positive/Negative。
    \begin{center}
        \begin{tabular}{lll}
                     & True & False \\
            Positive & TP   & FP    \\
            Negative & TN   & FN    \\
        \end{tabular}
    \end{center}
    \begin{definition}{精确率、召回率、F1}
        \begin{eqnarray}
            precision & = & TP / (TP + FP) \nonumber\\
            recall & = & TP / (TP + FN) \nonumber\\
            F1 & = & \frac{precision\cdot recall}{precision+recall}\nonumber
        \end{eqnarray}
    \end{definition}
    分别为 预测阳性中真实为正的概率、样本中的正例有多少被预测正确、以及二者的调和平均。
\end{frame}

\begin{frame}[fragile]
    \frametitle{评价机器学习效果的指标}
    除此之外，我们再来比较一下“相关系数”，看一看预测差异是否很大。
    \begin{minted}{python}
def get_score(Xtest, Ytrue, model):
    Ypred = model(Xtest)
    avg = "weighted"
    rating_map = {i: ord(i[0]) * 100 - len(i)
                    for i in Y.unique()}
    return {
        "precision":
            precision_score(Ytrue, Ypred, average=avg),
        "recall": recall_score(Ytrue, Ypred, average=avg),
        "f1": f1_score(Ytrue, Ypred, average=avg),
        "\(R^2\)": pearsonr(
            [rating_map[i] for i in Ypred],
            [rating_map[i] for i in Ytest]
        )[0],
    }
\end{minted}
\end{frame}
\begin{frame}
    \frametitle{完全随机的情况}
    如果我们训练的分类器完全无效，那么结果如表\ref{random}所示。
    \begin{table}
        \caption{随机的情况}
        \begin{tabular}{ll}
            precision & 0.2364 \\
            recall    & 0.1254 \\
            f1        & 0.1544 \\
            \(R^2\)   & 0.0089 \\
        \end{tabular}
        \label{random}
    \end{table}
\end{frame}
\subsection{深度学习（神经网络）}
\subsubsection{多层感知机}
\begin{frame}
    \frametitle{Why Neural Networks can learn almost anything\footnote{\href{https://www.youtube.com/watch?v=0QczhVg5HaI}{youtube视频}}}
    函数是抽象现实世界的好方法，如 \(\displaystyle F=G\frac{Mm}{R^2}\)，但是我们经常遇到的是并不知道函数的解析形式而只知道某几个点的值。

    神经网络是源于对生物神经的模拟，是一种通用的函数估计方法\cite{hornik1989multilayer}，通过已知的点训练出神经网络 $f$ ，我们可以用它来预测未知的 $x$ 为 $f(x)$ ，尽管这个 $f$ 不一定是解析的。

    \begin{center}
        \href{https://playground.tensorflow.org/}{\includegraphics[width=0.7\linewidth]{../lib/tfplay.png}}
    \end{center}
\end{frame}

\begin{frame}
    \frametitle{梯度下降与反向传播}
    \begin{columns}
        \column{0.5\linewidth}
        \begin{figure}
            \centering
            \includegraphics[width=\linewidth]{../lib/lr.png}
            \label{learningrates}
            \caption{不同学习率的影响}
        \end{figure}
        \column{0.5\linewidth}
        先看最简单的线性的情况。假设有一个我们不知道 $w,b$ 的线性的 $y=wx+b$ ，仅知道若干个 $(x,y)$。

        首先定义 \textbf{损失函数} 为 $loss = \sum{(\hat{y}-y)^2}$ ，即 $loss(w, b)=\sum(wx+b-y)^2$ 。我们要最小化估计的损失函数。

        令 $w=w_0,b=b_0$ ，正向计算得到 $loss(w_0,b_0)$。
        为了最小化它，我们让 $w,b$ 沿着 $loss$ 的梯度走一小步（\textbf{learning rate}），即误差反向传播。
        不断迭代，逼近 $loss(w,b)$ 的最小值，得到估计的 $w^*,b^*$。
    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{梯度下降与反向传播}
    左边的代码是我们手动实现的上述过程，右边的代码是更加 pytorch 风格的代码。
    \begin{columns}
        \column{0.5\linewidth}
        \tiny\begin{minted}{python}
import torch 
x = torch.rand([500,1]) # 张量可以当成是特殊的向量
y_true = 3*x+8
learning_rate = 0.05 
# w b 自动求导
w = torch.tensor([[0]], requires_grad=True) 
b = torch.tensor(0, requires_grad=True
                    , dtype=torch.float32)
for i in range(500):
    y_pred = torch.matmul(x,w)+b # 预测值
    loss = (y_true-y_pred).pow(2).mean() # loss(w, b)
    if w.grad is not None: # 不清零会累加
        w.grad.data.zero_()
    if b.grad is not None:
        b.grad.data.zero_()
    loss.backward() # 反向传播得到梯度
    w.data = w.data - w.grad*learning_rate
    b.data = b.data - b.grad*learning_rate
    if i % 50 == 0:
        print(w.item(), b.item(), loss.item())
\end{minted}
        \column{0.5\linewidth}
        \tiny\begin{minted}{python}
import torch
from torch import nn,optim
class Lr(nn.Module):
    def __init__(self):
        super(Lr, self).__init__()
        self.layer = nn.Linear(1,1)
    def forward(self, x):
        return self.layer(x)
x = torch.rand([500,1])
y = 3*x+8
model = Lr()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.05)
for i in range(500):
    out = model(x)
    loss = criterion(y, out)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
list(model.parameters())
        \end{minted}
    \end{columns}
\end{frame}
\begin{frame}
    \frametitle{激活函数}
    \begin{columns}
        \column{0.5\linewidth}
        \begin{figure}
            \includegraphics[width=\linewidth]{../lib/neuron.png}
            \caption{生物神经的动作电位}
        \end{figure}

        \column{0.5\linewidth}
        线性函数的叠加仍然是线性函数。为了实现对非线形特征的识别，也为了对生物神经有更好的模拟，神经网络中每个神经元的输入添加了非线性的“\textbf{激活函数}”。常见的有：

        \begin{eqnarray}
            ReLU(x)&=&\max(0,x)\\
            Sigmoid(x)&=&\frac {1}{1+e^{-x}}\\
            tanh(x)&=&\tanh{x}\\
            Softmax(x_i)&=&\frac{e^{x_i}}{\sum e^{x_j}}
        \end{eqnarray}

    \end{columns}
\end{frame}

\begin{frame}[fragile]
    \frametitle{神经网络}
    一层神经元的输出，作为另一层神经网络的输入，就形成了一层神经网络。多层神经网络通过损失函数和优化器迭代训练，就形成了一个简单的感知机。

    \begin{columns}
        \column{0.5\linewidth}
        \tiny\begin{minted}{python}
from torch import nn
import torch
Ytrain_nn = torch.tensor(pd.get_dummies(Ytrain).values, 
                dtype=torch.float32)
Xtrain_nn = torch.tensor(Xtrain.values, dtype=torch.float32)
net = nn.Sequential(
    nn.Linear(Xtrain_nn.shape[1], 40),
    nn.ReLU(),
    nn.Linear(40, 7),
    nn.Softmax(dim=1)
)
optimizer = torch.optim.SGD(net.parameters(), lr=0.001)
loss_func = torch.nn.MSELoss()
for t in range(10000):
    prediction = net(Xtrain_nn)
    loss = loss_func(Ytrain_nn, prediction)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
Xtest_nn = torch.tensor(Xtest.values, dtype=torch.float32)
prediction = pd.DataFrame(net(Xtest_nn).detach().numpy())           
        \end{minted}
        \column{0.4\linewidth}
        \begin{table}
            \caption{BP 神经网络结果}
            \begin{tabular}{ll}
                precision & 0.3035 \\
                recall    & 0.3267 \\
                f1        & 0.2913 \\
                \(R^2\)   & 0.0463 \\
            \end{tabular}
            \label{BP}
        \end{table}
    \end{columns}
\end{frame}
\subsubsection{CNN}
\begin{frame}
    \frametitle{卷积}
    \begin{definition}{卷积}
        \begin{equation}
            (f*g)(t)=\int_{\mathbb{R}^n}f(\tau)g(t-\tau)\mathrm{d}\tau
        \end{equation}
    \end{definition}
    \begin{columns}
        \column{0.4\linewidth}
        卷积神经网络最早应用在图像识别领域，利用“做卷积”扫描图像上的 (R,G,B) 元组得到像素点附近的一层特征。

        当然这样的特征可能会导致过拟合，扫描的结果会经过“pooling”的过程减少参数和计算量，防止过拟合，提高模型泛化能力。

        最后通过一层全连接层将CNN的结果输出出来。
        \column{0.5\linewidth}
        \begin{figure}
            \includegraphics[width=\linewidth]{../lib/CNN.jpeg}
            \caption{卷积}
            \label{CNN}
        \end{figure}
    \end{columns}

\end{frame}
\begin{frame}[fragile]
    \frametitle{实现 CNN}
    \begin{columns}
        \column{0.6\linewidth}
        \begin{minted}{python}
class CNN(nn.Module):
    def __init__(self) -> None:
        super(CNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv1d(48, 20, 3,padding=2),
            nn.Tanh(),
            nn.AvgPool1d(3),
        )
        self.fc = nn.Sequential(
            nn.Linear(20, len(encode)),
            nn.ReLU(),
            nn.Softmax(dim=1),
        )

    def forward(self, x):
        out = self.conv(x).view(out.size(0), -1)
        return self.fc(out)
        \end{minted}
        \column{0.3\linewidth}
        \begin{minted}{python}
net = CNN()
optimizer = torch.optim.Adamax(net.parameters(),lr=0.0025)
loss_func = torch.nn.L1Loss()
        \end{minted}
        \begin{table}
            \caption{CNN 结果}
            \begin{tabular}{ll}
                precision & 0.4027 \\
                recall    & 0.4350 \\
                f1        & 0.4140 \\
                \(R^2\)   & 0.4039 \\
            \end{tabular}
            \label{CNNres}
        \end{table}
    \end{columns}
\end{frame}
\subsubsection{RNN}
\begin{frame}
    \frametitle{“记忆”}
\end{frame}
\begin{frame}
    \frametitle{实现 LSTM}
\end{frame}
\begin{frame}
    \frametitle{其他神经网络}
\end{frame}
\subsection{经典机器学习算法}
\subsubsection{从logit模型开始}
\begin{frame}[fragile]
    \frametitle{logit 模型}
    关于分类我们自然地想到了 logit 回归。我们不妨以 logit 回归为切入点看一看 sklearn 是如何训练模型的：

    \begin{minted}{python}
from sklearn.linear_model import LogisticRegression
logit = LogisticRegression(solver="saga", 
            multi_class="multinomial", random_state=42)
logit.fit(Xtrain, Ytrain)
logit.predict(Xtest)
    \end{minted}

    \begin{table}
        \caption{logit 回归}
        \begin{tabular}{ll}
            precision & 0.1815  \\
            recall    & 0.2440  \\
            f1        & 0.1547  \\
            \(R^2\)   & -0.0177 \\
        \end{tabular}
        \label{logit}
    \end{table}
\end{frame}
\subsubsection{基于树的算法}
\begin{frame}[fragile]
    \frametitle{决策树}

    决策树直观上很好理解，也是我们今天少数可解释的模型。一个数据集有多个特征，每个节点按照某个特征是否满足一定的条件分叉，形成一棵二叉树。

    该节点选取特征分叉的决策依据是最大化“信息增益”，即分叉前后数据更“有序”，且更有序的程度最大（信息熵变化最大）。
    这棵树为了避免过拟合，我们会对决策树“剪枝”，增加一些分支条件的限制，可以看\href{https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html}{sklearn 文档}。
    \begin{center}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{/Users/dcy/Desktop/thesis/data/decision_tree.png}
            \label{decisiontree}
            \caption{决策树}
        \end{figure}
    \end{center}
\end{frame}
\begin{frame}
    \frametitle{集成学习}
    决策树一般是一种比较弱的分类器。集成学习则是利用多个弱分类器的集成，形成一个强分类器。

    组合的方法常见的有两种：bagging 和 boosting。bagging 是平行地训练弱分类器然后投票，特点是不容易过拟合。
    典型的随机森林就是随机地选取样本和特征训练出一棵棵决策树后投票，如图 \ref{randomforest} 所示。
    \begin{center}
        \begin{figure}
            \includegraphics[width=0.8\linewidth]{../lib/rf.png}
            \label{randomforest}
            \caption{随机森林}
        \end{figure}
    \end{center}
\end{frame}
\begin{frame}
    \frametitle{集成学习}
    使用 boosting 的梯度提升树可以树的深度很少就能达到很高的精度。
    boosting 迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关，直观比方是每个树都去学习上一个树没有学习好的地方。
    \begin{center}
        \begin{figure}
            \includegraphics[width=0.6\linewidth]{../lib/boosting.png}
            \label{boosting}
            \caption{梯度提升树}
        \end{figure}
    \end{center}
\end{frame}
\begin{frame}[fragile]
    \frametitle{基于树的算法代码}
    \begin{minted}{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(Xtrain, Ytrain)
rf = RandomForestClassifier(max_depth=4, random_state=42)
rf.fit(Xtrain, Ytrain)
gb = GradientBoostingClassifier(max_depth=3, random_state=42)
gb.fit(Xtrain, Ytrain)
    \end{minted}
    \begin{table}
        \caption{基于树的算法结果}
        \begin{tabular}{l|cccc}
            model & precision & recall & f1     & \(R^2\) \\ \hline
            决策树   & 0.3498    & 0.3799 & 0.3529 & 0.3632  \\
            随机森林  & 0.3960    & 0.4251 & 0.3835 & 0.3995  \\
            梯度提升树 & 0.5305    & 0.5256 & 0.5095 & 0.5421  \\
        \end{tabular}
        \label{trees}
    \end{table}
\end{frame}
\subsubsection{基于距离的算法}
\begin{frame}
    \frametitle{支持向量机}
    SVM 的思想是样本分布在空间中，找到一个可以恰好划分开样本点、并且间隔最大的的（超）平面。

    当然这样的超平面不一定好找，我们可以用加入惩罚项的“软间隔”优化或者利用“核函数”将空间映射为可分的。

    \begin{center}
        \begin{figure}
            \includegraphics[width=0.6\linewidth]{../lib/SVM.jpeg}
            \caption{Support Vector Machine}
            \label{SVM}
        \end{figure}
    \end{center}
\end{frame}
\begin{frame}
    \frametitle{KNN 与 K Means}
    \begin{columns}
        \column{0.5\linewidth}
        KNN 的思想是一个样本归属的分类属于离他最近的 K 个已经打好标签的邻居中占多数的分类，也非常直观。
        \column{0.5\linewidth}
        K Means 则是一种聚类算法，不需要打好标签（即所谓“无监督学习”），目的是把样本点归为 K 个群落。
        其思想是假设 K 个重心，样本根据与不同重心的距离归类，进而得到新的重心。
        以此循环往复，直至重心稳定下来。
    \end{columns}
\end{frame}
\begin{frame}[fragile]
    \frametitle{基于距离的算法实现}
    \begin{minted}{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

svm = SVC(kernel="rbf", gamma="auto", random_state=42)
svm.fit(Xtrain, Ytrain)
KNN = KNeighborsClassifier(n_neighbors=3)
KNN.fit(Xtrain, Ytrain)
    \end{minted}
    \begin{table}
        \caption{基于距离的算法结果}
        \begin{tabular}{l|cccc}
            model & precision & recall & f1     & \(R^2\) \\ \hline
            SVM   & 0.4137    & 0.4094 & 0.3517 & 0.3431  \\
            KNN   & 0.3625    & 0.3523 & 0.3420 & 0.2987  \\
        \end{tabular}
        \label{distances}
    \end{table}
\end{frame}
\subsection{对比}
\begin{frame}
    \frametitle{结果对比}
    \centering
    \begin{table}
        \caption{机器学习效果对比表}
        \label{result}
        \begin{tabular}{l|cccc}
            model    & precision & recall & f1     & \(R^2\) \\\hline
            random   & 0.2364    & 0.1255 & 0.1544 & 0.0089  \\
            logit    & 0.1815    & 0.2441 & 0.1547 & -0.0178 \\
            tree     & 0.3499    & 0.3799 & 0.3529 & 0.3632  \\
            bagging  & 0.3960    & 0.4252 & 0.3835 & 0.3996  \\
            boosting & 0.5305    & 0.5256 & 0.5095 & 0.5421  \\
            svm      & 0.4137    & 0.4094 & 0.3517 & 0.3431  \\
            KNN      & 0.3625    & 0.3524 & 0.342  & 0.2987  \\
            BP       & 0.3035    & 0.3267 & 0.2913 & 0.0463  \\
            CNN      & 0.4027    & 0.4350 & 0.4140 & 0.4039  \\
            RNN      & 0.4516    & 0.4606 & 0.4541 & 0.4176  \\
        \end{tabular}
    \end{table}

\end{frame}
\begin{frame}
    \frametitle{结果对比}
    \centering
    \begin{figure}
        \includegraphics[width=0.5\linewidth]{/Users/dcy/code/ernest/static/ox-hugo/529497edec8df5a4871a4e92910dc24d9e9f45dc.png}
        \caption{机器学习效果对比图}
        \label{contrast}
    \end{figure}
\end{frame}
\section{机器学习在企业风险管理中的应用}

\begin{frame}
    \frametitle{机器学习在企业风险管理中的应用}
    \Textcite{mai2019deep} 利用 CNN 预测企业破产，在处理文本数据时利用 word embedding 量化，AUC 曲线如图
    \begin{figure}
        \includegraphics[width=\linewidth]{../lib/mlinerm.jpg}
        \caption{AUC曲线}
    \end{figure}
\end{frame}
\begin{frame}
    \frametitle{机器学习在企业风险管理中的应用}
    \Textcite{kellner2022opening} 利用神经网络预测违约损失 Loss Given Default。

    他们将传统的分位数回归的回归元作为第一层，通过神经网络揭示其中的非线性关系，比如交叉项及其他非线性关系，神经网络最后一层是传统的分位数回归。利用 first order feature importance，量化输入变量的整体重要性。同时排除掉二阶的和交互的在分位数中接近于零。因此 QRNN 和分位数 QR 的分位数损失非常相似
    通过允许分位数回归神经网络实现的分位数中的非线性和相互作用来扩展这种方法。这种方法大大增强了建模的灵活性。额外的灵活性在更好地分布拟合和超时样本方面带来了回报，分位数预测精度提高了 30\%，同时更加 robust 。
\end{frame}
\begin{frame}
    \frametitle{机器学习在企业风险管理中的应用}
    \Textcite{golbayani2020comparative}
    使用决策树、随机森林、支持向量机和多层感知器应用于相同的数据集，预测公司未来评级。他们统计了机器学习在债券评级和公司信用评级方面的文章，很多认为 SVM 和神经网络是比较准确的。但是他们使用 Notches Distance 来对机器学习绩效来打分，认为基于决策树的两种方法更有效。

    当前机器学习最火热的两个应用方向是计算机视觉 CV 和自然语言处理 NLP ，亦有一些文献利用自然语言处理分析文本数据做研究。
\end{frame}
\section{参考文献}
\begin{frame}{参考文献}
    \printbibliography
\end{frame}
\end{document}
